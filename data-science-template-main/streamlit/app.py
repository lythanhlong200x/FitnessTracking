import json
import joblib
import pandas as pd
import requests
import streamlit as st
from build_data import perform_clustering, preprocess_data
from count_rep import count_reps_and_evaluate
from read_file import read_data_from_files, resample_and_save_data
from run_model import run_model_and_save_predictions
import streamlit.components.v1 as components

# Set page title and layout
st.set_page_config(page_title="Fitness Tracking Data Processing App üí™", layout="wide")


def load_lottieurl(url: str):
    r = requests.get(url)
    if r.status_code != 200:
        return None
    return r.json()


def load_lottiefile(filepath: str):
    with open(filepath, "r") as f:
        return json.load(f)

    # Page title


st.markdown(
    """
<style>
.centered-title {
    text-align: center;
}
    </style>
""",
    unsafe_allow_html=True,
)
st.markdown(
    '<h1 class="centered-title">Fitness Tracking Data Processing App</h1>',
    unsafe_allow_html=True,
)
lottie_coding = load_lottiefile(
    "./data-science-template-main/streamlit/images/fitness.json"
)
# st_lottie(lottie_coding)

header_image = "./data-science-template-main/streamlit/images/exercises.jpg"
# header_image = "../streamlit/images/exercises.jpg"

# ƒê∆∞·ªùng d·∫´n ƒë·∫øn h√¨nh ·∫£nh header
st.image(header_image, use_column_width=True)


st.write("## üóà Note: L·∫•y d·ªØ li·ªáu t·ª´ ·ª©ng d·ª•ng Phyphox ")
st.markdown(
    "[![Foo](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRP66YjvR6u01MAM-jSK_0F4Jy_Fn3u7AZZtGtKSIp1&s)](http://google.com.au/)"
)

with st.expander("‚ú® H∆∞·ªõng d·∫´n c√°ch l·∫•y d·ªØ li·ªáu g·ªëc"):
    """
    üåüüåü B·∫°n c√≥ th·ªÉ d√πng ƒëi·ªán tho·∫°i c·ªßa m√¨nh, ho·∫∑c k·∫øt n·ªëi ƒë·ªìng h·ªì th√¥ng minh c√≥ kh·∫£ nƒÉng thu th·∫≠p 
    data v·ªÅ Gia t·ªëc v√† Con quay h·ªìi chuy·ªÉn th√¥ng qua app Phyphox. H√£y th·ª±c hi·ªán theo c√°c b∆∞·ªõc sau ƒë√¢y:

    1. **T·∫£i Xu·ªëng Phyphox**: Truy c·∫≠p [trang web Phyphox](https://phyphox.org/) ho·∫∑c t·∫£i ·ª©ng d·ª•ng Phyphox t·ª´ Appstore ho·∫∑c Google Play.
    
    2. **M·ªü Phyphox**: Kh·ªüi ƒë·ªông ·ª©ng d·ª•ng Phyphox tr√™n ƒëi·ªán tho·∫°i th√¥ng minh c·ªßa b·∫°n.
    
    3. **Ch·ªçn th√≠ nghi·ªám**: Ch·ªçn th√≠ nghi·ªám b·∫°n mu·ªën th·ª±c hi·ªán t·ª´ danh s√°ch c√°c th·ª±c nghi·ªám c√≥ s·∫µn, ·ªü ƒë√¢y l√† Con quay h·ªìi chuy·ªÉn v√† Gia t·ªëc (kh√¥ng g)
    
    4. **Ti·∫øn h√†nh th√≠ nghi·ªám**: Tu√¢n theo c√°c h∆∞·ªõng d·∫´n ƒë∆∞·ª£c cung c·∫•p trong ·ª©ng d·ª•ng ƒë·ªÉ thu th·∫≠p d·ªØ li·ªáu gia t·ªëc k·∫ø v√† gyro t·ªëc k·∫ø.
    
    5. **Xu·∫•t D·ªØ Li·ªáu**: Sau khi thu th·∫≠p d·ªØ li·ªáu ho√†n t·∫•t, ch·ªçn d·∫•u 3 ch·∫•m ·ªü g√≥c tr√™n ph·∫£i m√†n h√¨nh -> ch·ªçn xu·∫•t d·ªØ li·ªáu -> ch·ªçn CSV (Comma, decimal Point).
    
    6. **ƒê·ªãnh d·∫°ng t·ªáp tin**: Vui l√≤ng ƒë·∫∑t t√™n file theo c√∫ ph√°p "T√™n - Ch√∫ th√≠ch - Acc (v·ªõi Gia t·ªëc)/Gyr (v·ªõi Con quay)" VD: Hoang-1-acc.
    
    6. **T·∫£i L√™n D·ªØ Li·ªáu**: T·∫£i t·ªáp CSV ƒë√£ xu·∫•t l√™n ·ª©ng d·ª•ng n√†y ƒë·ªÉ x·ª≠ l√Ω, bao g·ªìm m·ªói set g·ªìm 2 file acceleration & gyroscope.
"""



uploaded_files = st.file_uploader(
    "Upload CSV files", type=["csv"], accept_multiple_files=True
)


# Main Streamlit function
def main():

    # Allow user to upload data file

    if uploaded_files:
        acc_df, gyr_df = read_data_from_files(uploaded_files)
        st.write("Processing uploaded files...")
        new_data, data_resample = view_data(acc_df, gyr_df)
        page_selection = st.sidebar.selectbox(
            "Select Options", ["View Data", "View Reps Count"]
        )

        if page_selection == "View Data":
            st.write("Resampling and saving processed data...")
            st.subheader("View Data After Preprocessed")
            st.subheader("Accelerometer Data:")
            st.dataframe(acc_df.head())
            st.subheader("Gyroscope Data:")
            st.dataframe(gyr_df.head())
            st.write("Data Resampled:")
            st.write(data_resample)
            st.write("Cluster File Content:")
            st.write(new_data)
        elif page_selection == "View Reps Count":
            view_reps_count(new_data, data_resample)


def view_data(acc_df, gyr_df):

    # Resample and save processed data
    data_resample = resample_and_save_data(acc_df, gyr_df)
    data_freq = preprocess_data(data_resample)
    data_resample = resample_and_save_data(acc_df, gyr_df)
    data_freq = preprocess_data(data_resample)
    new_data = perform_clustering(data_freq)
    return new_data,data_resample


def view_reps_count(new_data, data_resample):

    st.subheader("View Reps Count Section")
    loaded_model = joblib.load(
        "./data-science-template-main/streamlit/custom_random_forest_model.pkl"
    )
    df_train = pd.read_pickle(
        "./data-science-template-main/streamlit/TrainingData.pkl"
    )
    data_reps = run_model_and_save_predictions(
        df_train, data_resample, new_data, loaded_model
    )

    # Perform reps counting and evaluation
    st.write("Counting repetitions and evaluating results...")
    summary_by_exercise = count_reps_and_evaluate(data_reps)
    st.subheader("Summary by Exercise:")
    st.dataframe(summary_by_exercise)

    st.success("Processing complete!")


if __name__ == "__main__":
    try:
        main()
    
    except Exception as e:
        st.error("ƒê√£ x·∫£y ra l·ªói trong qu√° tr√¨nh ch·∫°y ·ª©ng d·ª•ng. Vui l√≤ng ki·ªÉm tra l·∫°i ƒë·ªãnh d·∫°ng file c·ªßa b·∫°n.")
        # st.write(e)
  
 
